%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
% 			\vspace{#2}
% 			\begin{center}
% 			Figure \thelecnum.#1:~#3
% 			\end{center}
% 	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
% \newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{Probabilistic Approximation of Loss Functions}{Abir De}{Kaustav Prasad, Ayush Kapoor, Harshit Shrivastava, Yash Sanjeev}
%\lecture{x}{Title}{Abir De}{Group y}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%. - Yash Sanjeev

\section{Choices for a Hinge Loss Function}
\textbf{Problem} \hspace{0.1em} Given data $\mathcal{D}\equiv\{(x_i, y_i)\}_{1\leq i\leq n}$ where $x\in \mathbb{R}^d$ and $y\in \{-1, 1\}$, the objective is to find $w\in \mathbb{R}^d$ such that $w^\top w = 1$ and $$w^\top x > 1 : y = 1 \text{\quad , \quad} w^\top x < -1 : y = -1$$
When $|w^\top x| < 1$, we can assign a loss function which is greater when $\text{sgn}(w^\top x)\ne y$.\hfill{$\square$} \\

\noindent
The problem translates to the situation where we wish to separate the data points with $y=1$ from the data with $y=-1$ using the line with normal vector $w$. The points with $|w^\top x| < 1$ are within the "margin" of the separator, and contribute to the loss regardless of the value of $y$. The constraint $w^\top w = 1$ is imposed to avoid linear scaling of $w$, to artificially decrease the width of the "margin". The most widespread choices for the loss function have been outlined below.\\

\noindent
\textbf{Solution} \hspace{0.1em}  The first choice of $w$ proposed is
$$\underset{w^\top w=1}{\min} \sum_{(x,y)\in\mathcal{D}}\exp(-y\cdot w^\top x)$$
Observe that
\begin{align*}
    L &= 0 \text{ when } y = 1 \text{ and } w^\top x > 1\\
    L &= 0 \text{ when } y = -1 \text{ and } w^\top x < -1
\end{align*}
so a prudent choice for the loss function is
$$\text{Loss}(w^\top x, y) = \max(0, 1 - y \cdot w^\top x)$$
also known as the hinge loss, useful in the training of Support Vector Machines (SVMs). \hfill{$\square$}\\

\noindent
\textbf{Note} \hspace{0.1em} Both the definitions of the loss functions over the entire dataset $\mathcal{D}$ give a measure of $$\sum_{(x,y)\in\mathcal{D}} \mathbb{I}\left(h(x)\ne y\right)$$
where $h: \mathbb{R}^d \longrightarrow \{-1, 1\}$ is given by $h(x) = \text{sgn}(w^\top x)$




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%. - Harshit Shrivastava
\section{Probabilistic Approximation of Loss Functions}
The probabilistic approach is a much more guided and principled one which provides us with some mathematical guarantee as will be shown later in the course. We try to interpret the same problem as above in a different manner. In this Probabilistic Approximation method, we assume that x and y are drawn from a probability distribution and are independent and identically distributed Random Variables.
\begin{align*}
     x &\sim P(.) \\
     y &\sim P(./x) \\
\end{align*}
Now we need to find a probabilistic way of approximating $$\sum_{(x,y)\in\mathcal{D}} \mathbb{I}\left(h(x)\ne y\right)$$
We know that, when N is very large, 
\begin{align*}
   \sum_{i=1}^{N}g\left(X_{i}\right) \text{converges to } \mathbb{E}[g(X_{i})]
\end{align*}
Similarly, 
\begin{align*}
    \frac{1}{|D|}\sum_{(x,y)\in\mathcal{D}} \mathbb{I}\left(h(x)\ne y\right) \text{converges almost surely to } \mathbb{E}[\mathbb{I}(h(x)\ne y)]  = P(h(X)\ne y)
\end{align*}
Which is nothing but the Misclassification Probability. \\
In most problems, we are given with $P_{x}(.)\sim x$ and we try to find a $\hat{y} \sim P_{y}(./x)$ such that the misclassification probability is minimised. This is not an explicit function like the h(.) discussed earlier but rather a probabilty density we shall find for an estimator given x such that we achieve the same end goal as before, that is to minimise the loss, or equivalently in this case, misclassification probability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% - end of Harshit's section
\subsection{Estimating $w$ from Labelled Data}

In order to estimate the vector $w$ from data, we shall make the following assumption:

\textbf{Assumption: }\textit{The labelled data $D = \{ (x_i, y_i) \} $ is derived from a Probability Distribution with some fixed parameter $w^*$.}

For example, let us state that the probability distribution that generates the data is of the form

$$P(y | x) = \frac{1}{1 + e^{-w^{*T}x \cdot y}} \quad y \in \{-1, 1\} $$

Suppose that we have generated some data i.e a set of pairs $(x_i, y_i)$ from the distribution above. We wish to estimate $w^*$ by \textit{only} using the generated data, and presume no \textit{a priori} knowledge of $w*$. This can be accomplished by using the \textbf{Maximum Likelihood Estimator} on the paramter $w$ for the same distribution.

Let

$$P(y | x)_w = \frac{1}{1 + e^{-w^{T}x \cdot y}} \quad y \in \{-1, 1\} $$

We have taken the sample distribution to be the same as the distribution which generated the data, but in this case $w$ is an unknown parameter.

The Maximum Likelihood Estimate for $w$ is the value of $w$ for which

$$\sum_{x,y \in D} log \, P_w(y | x)$$

is maximized. Let us write the value of $w$ obtained in this way as $\hat{w}$.



\begin{flushleft}
Q. \textbf{What is the relationship between $\hat{w}$ and $w^*$?}\\

Ans. \color{blue} Let us say that we derive $n$ independent data sets from the $w*$ parametrized distribution and apply MLE to each of those data sets to obtain a set $\hat{w_1}, \hat{w_2} \cdots \hat{w_n}$ of estimates. Then, the expectation value of the members of this set is $w*$, i.e.

$$\mathbb{E}[\hat{w_i}] = w^*$$
\end{flushleft}

\subsection{Use of $w$ in estimating the $P(y|x)$ for (x,y) \not \in $D$}
For given $(x_i,y_i)$ part of data set D, we use \textbf{Maximum Likelihood Estimator(MLE)} to estimate $\hat{w}$
for that particular data set D as the value of $w$ for which underlying expression is maximum. 
$$\sum_{x,y \in D} log \, P_w(y | x) \quad where \hspace{7}P_w(y | x) = \frac{1}{1 + e^{-w^{*T}x \cdot y}}   $$ 
And consequently if we consider n independent data sets to obtain $\hat{w_1}, \hat{w_2} \cdots \hat{w_n}$ and using those values determine the expectation value as $$\mathbb{E}[\hat{w_i}] = w^*$$
As expectation value provides the minimum variance from the set of values  $\hat{w_1}, \hat{w_2} \cdots \hat{w_n}$.
When a probability for a new pair of (x, y) is evaluated, w* will be the best approximation because it is least biased towards any $\hat{w_i}$.

 $$min \hspace{4} \mathbb{E}(||\hat{w_i} - w||^2) \hspace{1},\hspace{10}when \hspace{10} w = w^* $$

And consequently, we can estimate the probability by

$$ \hat{P} \approx  P(y | x)_w* = \frac{1}{1 + e^{-w^{*T}x \cdot y}} \quad y \in \{-1, 1\} $$


\section{Group Details and Individual Contribution}
\textbf{Yash Sanjeev (180070068): }Wrote Section 3.1 (Choices for a Hinge Loss Function).\\
\textbf{Harshit Shrivastava (18D070011):} Wrote Section 3.2 (Probabilistic Approximation of Loss Function).\\
\textbf{Kaustav Prasad (200260024): }Wrote Section 3.2.1 (Estimating $w$ from Labelled Data)\\
\textbf{Ayush Kapoor (200020038): }Wrote Section 3.2.2 (Use of $w$ in estimating the $P(y|x)$ for (x,y) \not \in $D$)


\end{document}





